# -*- coding: utf-8 -*-
"""Modeling3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1N__VZZRHLeaLsPNkuDws-_qTbZHIbMun

# Group 9 Term project
### 미국 구인공고 웹사이트 Glassdoor 직업 관련 데이터

목표:

산업별 또는 평점별 분포 분석, 급여 정규화 및 클러스터링, 회귀모델 등의 후속 분석을 통해 다음과 같은 결과를 얻고자 합니다.

-	기업의 규모, 업종, 소유 형태, 위치 등과 급여의 상관관계 분석
"""

# 라이브러리 import
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split, KFold, GridSearchCV, RandomizedSearchCV, cross_val_score
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from xgboost import XGBRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

"""# 1. 데이터 불러오기

[Data Science Jobs & Salaries 2024]
https://www.kaggle.com/datasets/fahadrehman07/data-science-jobs-and-salary-glassdoor?select=glassdoor_jobs.csv

### 데이터 셋 개요 (피처와 설명)
- Job Title	: 직책명
- Salary Estimate :	회사가 제공하는 직무에 대한 예상 급여
- Job Description	: 직무 설명
- Rating : 회사 평가
- Company Name : 회사명
- Location	: 작업 위치
- Headquarters	: 회사 본사
- Size	: 회사의 직원 수
- Founded	: 회사가 설립된 연도
- Type of ownership	: 민간, 공공, 정부 및 비영리 조직과 같은 소유권 유형
- Industry :	Aerospace, Energy 등 회사가 서비스를 제공하는 산업 유형
- Sector	: 산업(에너지), 부문(석유, 가스)와 같이 어떤 유형의 서비스를 제공하는지
- Revenue	: 회사의 총 수익
- Competitors	: 회사 경쟁사
"""

# 데이터 로드 및 'avg_salary' 결측치 제거
df = pd.read_csv("glassdoor_cleaned_final.csv")
df = df.dropna(subset=['avg_salary'])

"""# 2. 데이터 탐색 (EDA)"""

# 각 범주형 변수에 따른 평균 급여 출력
print(df.groupby('Size_cleaned')['avg_salary'].mean())
print(df.groupby('Ownership_Grouped')['avg_salary'].mean())
print(df.groupby('Industry')['avg_salary'].mean().sort_values(ascending=False).head(10))

# 수치형 변수 간의 상관관계 시각화
sns.heatmap(df[['Company_age', 'Rating', 'avg_salary']].corr(), annot=True)
plt.title("Correlation Between Numerical Variables")
plt.show()

# 기업 규모별 급여 분포 시각화
sns.boxplot(data=df, x='Size_cleaned', y='avg_salary')
plt.xticks(rotation=45)
plt.title("Salary Distribution by Company Size")
plt.show()

# 본사 위치에 따른 급여 분포 비교 (상위 10개 위치 기준)
top_states = df['Headquarters_state_binned'].value_counts().head(10).index
sns.boxplot(data=df[df['Headquarters_state_binned'].isin(top_states)],
            x='Headquarters_state_binned', y='avg_salary')
plt.xticks(rotation=45)
plt.title("Salary Distribution by Headquarters Location")
plt.show()

# 산업별 기업 나이와 급여의 관계 (다변량 시각화)
sns.lmplot(data=df, x='Company_age', y='avg_salary', hue='Sector', fit_reg=False)
plt.title("Company Age and Salary – Industry-wise Distribution")
plt.show()

"""# 3. 기초 피처 엔지니어링
결측치 처리, 스케일링 및 인코딩
"""

# 분석에 사용할 feature 및 target 설정
features = ['Size_cleaned', 'Ownership_Grouped', 'Industry', 'Sector',
            'Location_state_binned', 'Headquarters_state_binned',
            'works_at_headquarters', 'Company_age', 'Rating']
target = 'avg_salary'

X = df[features]
y = df[target]

# 수치형, 범주형 변수 구분
numeric_features = ['Company_age', 'Rating']
categorical_features = list(set(features) - set(numeric_features))

# 수치형 변수 전처리: 결측치 평균 대체 후 표준화
num_imputer = SimpleImputer(strategy='mean')
num_scaler = StandardScaler()
X_num = num_scaler.fit_transform(num_imputer.fit_transform(X[numeric_features]))

# 범주형 변수 전처리: 결측치 최빈값 대체 후 one-hot 인코딩
cat_imputer = SimpleImputer(strategy='most_frequent')
X_cat_imputed = cat_imputer.fit_transform(X[categorical_features])
ohe = OneHotEncoder(handle_unknown='ignore', sparse_output=False)
X_cat = ohe.fit_transform(X_cat_imputed)

# 수치형 + 범주형 병합
from numpy import hstack
X_processed = hstack([X_num, X_cat])

"""# 4.  회기 모델 학습
- Random Forest
- Xgboost
- Gradient boosting

  회기모델 학습 및 평가 함수정의 (kfold corss validation :평가지표는 mae, mse,rscore활용)

"""

# 모델 성능 평가 함수 정의 (KFold 적용, MAE, MSE, RMSE, R2 계산)
def evaluate_model(model, X, y):
    kf = KFold(n_splits=5, shuffle=True, random_state=42)
    mae = -cross_val_score(model, X, y, cv=kf, scoring='neg_mean_absolute_error').mean()
    mse = -cross_val_score(model, X, y, cv=kf, scoring='neg_mean_squared_error').mean()
    rmse = np.sqrt(mse)
    r2 = cross_val_score(model, X, y, cv=kf, scoring='r2').mean()
    return mae, mse, rmse, r2


"""# 5. 하이퍼 파라미터 튜닝
- RandomForest 
- XGBoost
"""

# RandomForest 파라미터 조합 하이퍼파라미터 튜닝
rf_params_list = [
    {'n_estimators': 50, 'max_depth': 5, 'min_samples_split': 2},
    {'n_estimators': 100, 'max_depth': 10, 'min_samples_split': 5},
    {'n_estimators': 100, 'max_depth': None, 'min_samples_split': 2},
    {'n_estimators': 200, 'max_depth': 15, 'min_samples_split': 5},
    {'n_estimators': 150, 'max_depth': 20, 'min_samples_split': 10}
]

rf_results = []
for i, params in enumerate(rf_params_list, 1):
    model = RandomForestRegressor(**params, random_state=42)
    mae, mse, rmse, r2 = evaluate_model(model, X_processed, y)
    rf_results.append({
        'Model': 'RandomForest',
        'Combination': f'RF_Set_{i}',
        'Params': params,
        'MAE': mae, 'MSE': mse, 'RMSE': rmse, 'R2': r2
    })

# XGBoost 파라미터 조합 하이퍼파라미터 튜닝 
xgb_params_list = [
    {'n_estimators': 50, 'learning_rate': 0.05, 'max_depth': 3},
    {'n_estimators': 100, 'learning_rate': 0.1, 'max_depth': 5},
    {'n_estimators': 100, 'learning_rate': 0.2, 'max_depth': 7},
    {'n_estimators': 200, 'learning_rate': 0.1, 'max_depth': 3},
    {'n_estimators': 150, 'learning_rate': 0.05, 'max_depth': 4}
]

xgb_results = []
for i, params in enumerate(xgb_params_list, 1):
    model = XGBRegressor(**params, random_state=42, verbosity=0)
    mae, mse, rmse, r2 = evaluate_model(model, X_processed, y)
    xgb_results.append({
        'Model': 'XGBoost',
        'Combination': f'XGB_Set_{i}',
        'Params': params,
        'MAE': mae, 'MSE': mse, 'RMSE': rmse, 'R2': r2
    })

# 결과 비교 및 최종 조합 선정
results_df = pd.DataFrame(rf_results + xgb_results)
best_result = results_df.loc[results_df['MAE'].idxmin()]

print("\nModel Performance Comparison (Sorted by MAE)")
print(results_df[['Model', 'Combination', 'MAE', 'RMSE', 'R2']].sort_values(by='MAE'))

print("\nBest Performing Model and Parameters:")
print(f"Model: {best_result['Model']}")
print(f"Combination: {best_result['Combination']}")
print(f"Hyperparameters: {best_result['Params']}")
print(f"MAE: {best_result['MAE']:.2f}, RMSE: {best_result['RMSE']:.2f}, R2: {best_result['R2']:.2f}")

# 최종 모델로 학습 및 중요 변수 시각화
if best_result['Model'] == 'RandomForest':
    final_model = RandomForestRegressor(**best_result['Params'], random_state=42)
else:
    final_model = XGBRegressor(**best_result['Params'], random_state=42, verbosity=0)

final_model.fit(X_processed, y)

importances = final_model.feature_importances_
feature_names = numeric_features + list(ohe.get_feature_names_out(categorical_features))
importance_df = pd.DataFrame({
    'Feature': feature_names,
    'Importance': importances
}).sort_values(by='Importance', ascending=False)

sns.barplot(data=importance_df.head(15), x='Importance', y='Feature')
plt.title(f"Top 15 Important Features by {best_result['Model']}")
plt.show()
